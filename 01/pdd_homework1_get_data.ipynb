{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------#\n",
    "#ALGORITHMS TO DO THE FOLLOWING TASKS. It lasted 5 or 6 hours to get all necessary data from this webpages.         #\n",
    "#                                                                                                                   #\n",
    "#  1. Browse the web https://dspace.cvut.cz/?locale-attribute=en and find out how to download data on               #\n",
    "#     Bachelor and Master theses.                                                                                   #\n",
    "#  2. Download or scrape the data such that for each thesis you know the following: Faculty name, department name,  #\n",
    "#     thesis title, thesis type (bachelor/master), supervisor name, reviewer name, year (or date) of the defence,   #\n",
    "#     study programme and discipline, link to a webpage with details.                                               #\n",
    "#  3. Store these data in one _csv_ file (should be handed in along with this notebook).                            #\n",
    "#-------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skit\n",
    "import seaborn as sns\n",
    "import requests\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------------------------#\n",
    "#--------FUNCTIONS--------FUNCTIONS--------FUNCTIONS--------FUNCTIONS--------FUNCTIONS--------FUNCTIONS--------#\n",
    "#--------------------------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def getTagFromPage(url, set_number) : #set number is to get informations from special part of html\n",
    "    #we get data about webpage with given url\n",
    "    html_page = urlopen(url)\n",
    "    soup = BeautifulSoup(html_page)\n",
    "\n",
    "    #type is bs4.element.ResultSet\n",
    "    result_set = soup.findAll('ul', attrs={'class' : 'ds-artifact-list list-unstyled'})\n",
    "\n",
    "    if len(result_set)>0 :\n",
    "        #type is bs4.element.Tag\n",
    "        return result_set[set_number]\n",
    "    else :\n",
    "        return None\n",
    "    \n",
    "def takeValueOfColumnFromDataFrame(data_frame_name, column_name) :\n",
    "    try :\n",
    "        value = data_frame_name[column_name].values[0]\n",
    "        return value\n",
    "\n",
    "    except KeyError :\n",
    "        return \"N/A\"\n",
    "\n",
    "def putDataIntoFrame(th,data_frame) :\n",
    "    #getting values\n",
    "    name = th.get(\"name\")\n",
    "    link = th.get(\"link\")\n",
    "    th_type = th.get(\"type\")\n",
    "    fac = th.get(\"fac_name\")\n",
    "    dep = th.get(\"dep_name\")\n",
    "\n",
    "    #preparing record\n",
    "    r=requests.get(link)        \n",
    "    r.encoding='utf-8'\n",
    "    #cp1250\n",
    "    #r.encoding='ISO8859_2'\n",
    "    ldf = pd.read_html(r.text, flavor='html5lib')\n",
    "    df = ldf[0]        \n",
    "    df.set_index([0], inplace=True)\n",
    "    dfT = df.transpose()\n",
    "    dfT = dfT.drop(dfT.index[1])            \n",
    "\n",
    "    #prepare data to insert    \n",
    "    supervisor = takeValueOfColumnFromDataFrame(dfT, \"dc.contributor.advisor\") #supervisor\n",
    "    reviewer  = takeValueOfColumnFromDataFrame(dfT, \"dc.contributor.referee\") #reviewer\n",
    "    date_of_def = takeValueOfColumnFromDataFrame(dfT, \"dc.date.issued\") #date of defece\n",
    "    s_prog = takeValueOfColumnFromDataFrame(dfT, \"theses.degree.programme\") #study programme\n",
    "    s_disc = takeValueOfColumnFromDataFrame(dfT, \"theses.degree.discipline\") #study discipline\n",
    "\n",
    "    #inserting\n",
    "    df_temp = pd.DataFrame([[fac,dep,name,th_type,supervisor,reviewer,date_of_def,s_prog,s_disc,link]], columns=listOfColumns)\n",
    "    data_frame = pd.concat([data_frame,df_temp],ignore_index=True)\n",
    "    \n",
    "    return data_frame \n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------------------------#\n",
    "#-----END OF FUNCTIONS-----END OF FUNCTIONS-----END OF FUNCTIONS-----END OF FUNCTIONS-----END OF FUNCTIONS-----#\n",
    "#--------------------------------------------------------------------------------------------------------------#\n",
    "#--------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "### GETTING DATA ABOUT FACULTY NAME, DEPARTMENT NAME AND TYPE OF THESIS ###\n",
    "\n",
    "#url from which we will start to search\n",
    "url = 'https://dspace.cvut.cz/handle/10467/3641'\n",
    "\n",
    "#each address contains this\n",
    "mainPartOfAddress = \"https://dspace.cvut.cz\"\n",
    "    \n",
    "#get html of main page\n",
    "tag_with_faculties = getTagFromPage(url, 0)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Start of algorithm. Time to get data about faculties' names!!!\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "#to store name of faculty\n",
    "faculties = []\n",
    "\n",
    "for link in tag_with_faculties.findAll('a') :\n",
    "    \n",
    "    temp_addr = mainPartOfAddress + link.get('href')\n",
    "    \n",
    "    faculty = {\n",
    "        \"name\" : link.string,\n",
    "        \"link\" : temp_addr\n",
    "    }\n",
    "    faculties.append(faculty)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Time to get data about departments' names!!!\")\n",
    "print()\n",
    "print()\n",
    "    \n",
    "#to store name of department\n",
    "departments = []  \n",
    "for faculty in faculties :\n",
    "    \n",
    "    fac_name = faculty.get(\"name\")\n",
    "    fac_link = faculty.get(\"link\")\n",
    "    \n",
    "    tag_with_departments = getTagFromPage(fac_link, 0)\n",
    "    \n",
    "    if tag_with_departments is None:\n",
    "        continue \n",
    "\n",
    "    for link in tag_with_departments.findAll('a') :\n",
    "        temp_addr = mainPartOfAddress + link.get('href')\n",
    "\n",
    "        department = {\n",
    "            \"name\" : link.string,\n",
    "            \"link\" : temp_addr,\n",
    "            \"fac_name\" : fac_name\n",
    "        }\n",
    "        departments.append(department)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Time to get data about types of theses!!!\")\n",
    "print()\n",
    "print()\n",
    "        \n",
    "#to store type of thesis        \n",
    "thesis_types = []\n",
    "i=0 #just to confirm that algorith is working\n",
    "for department in departments :\n",
    "    \n",
    "    #-----------------------------------------#\n",
    "    #just to confirm that algoritm is working #\n",
    "    if i%10==0 :                              #\n",
    "        print(i)                              #\n",
    "    i+=1                                      #\n",
    "    #-----------------------------------------#\n",
    "    \n",
    "    dep_name = department.get(\"name\")\n",
    "    dep_link = department.get(\"link\")\n",
    "    fac_name = department.get(\"fac_name\")\n",
    "\n",
    "    tag_with_thesis_types = getTagFromPage(dep_link,0)\n",
    "\n",
    "    if tag_with_thesis_types is None:\n",
    "        continue \n",
    "\n",
    "    for link in tag_with_thesis_types.findAll('a') :\n",
    "        temp_str = link.string #to check type of thesis\n",
    "        \n",
    "        #we need to store informations only about Bachelor and Master Theses\n",
    "        if temp_str.startswith(\"Baka\") or temp_str.startswith(\"Diplomo\") :\n",
    "            if temp_str.startswith(\"Baka\") :\n",
    "                th_type = \"Bachelor\"\n",
    "            else :\n",
    "                th_type = \"Master\"\n",
    "\n",
    "            temp_addr = mainPartOfAddress + link.get('href')\n",
    "\n",
    "            thesis_type = {\n",
    "                \"type\" : th_type,\n",
    "                \"link\" : temp_addr,\n",
    "                \"fac_name\" : fac_name,\n",
    "                \"dep_name\" : dep_name\n",
    "            }\n",
    "            thesis_types.append(thesis_type)\n",
    "            \n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "### GETTING REST OF THE DATA ABOUT THESIS FROM THE OWN PAGE OF THESIS WITH THEIR DETAILS ###\n",
    "\n",
    "print()\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Now it is time to get final data about theses!!!\")\n",
    "print(\"------------------------------------------------\")\n",
    "print()\n",
    "\n",
    "#to store informations about theses\n",
    "theses = []\n",
    "listOfColumns = ['FACULTY_NAME','DEPARTMENT_NAME', 'THESIS_TITLE', 'THESIS_TYPE','SUPERVISOR_NAME','REVIEWER_NAME', 'DATE_OF_DEFENCE','STUDY_PROGRAMME','STUDY_DISCIPLINE','LINK_TO_WEBPAGE']\n",
    "data_frame = pd.DataFrame(columns=listOfColumns)\n",
    "\n",
    "file_to_save = \"pdd_1st_homework_wz_{}.csv\"\n",
    "\n",
    "i=1 # to write current number of thesis_type\n",
    "j=1 # to write current number of thesis\n",
    "\n",
    "for thesis_type in thesis_types : \n",
    "    \n",
    "    \n",
    "    print(i)\n",
    "    i+=1\n",
    "    \n",
    "    th_type = thesis_type.get(\"type\")\n",
    "    th_type_link = thesis_type.get(\"link\") + \"?offset={}\"\n",
    "    fac_name = thesis_type.get(\"fac_name\")\n",
    "    dep_name = thesis_type.get(\"dep_name\")\n",
    "    \n",
    "    offset = 0\n",
    "    \n",
    "    #on one page there is just 20 theses. I have to take them from all pages\n",
    "    while True :\n",
    "        \n",
    "        temp_th_type_link = th_type_link.format(offset)\n",
    "        \n",
    "        tag_with_theses = getTagFromPage(temp_th_type_link,1)\n",
    "\n",
    "        if tag_with_theses is None:\n",
    "            break \n",
    "\n",
    "        #not to make infinitive loop. We want to check this pages where are records\n",
    "        num_of_th_on_page = 0\n",
    "            \n",
    "        for link in tag_with_theses.findAll('a') :\n",
    "            \n",
    "            #---------------------------------------------------------#\n",
    "            #just to confirm that algoritm is working                 #\n",
    "            if j%20 == 0 :                                            #\n",
    "                print(\"   {} link : {}\".format(j,temp_th_type_link))  #\n",
    "            #---------------------------------------------------------#\n",
    "            j+=1\n",
    "            \n",
    "            #every 1000 records I want to save them in case of any failure during the operation of algorithm\n",
    "            if j%1000==0 :\n",
    "                temp_filename = file_to_save.format(j)\n",
    "                data_frame.to_csv(temp_filename, sep='|', encoding='utf-8', index=False)\n",
    "                print(temp_filename)\n",
    "            \n",
    "            #address with details of thesis stored in a table\n",
    "            temp_addr = mainPartOfAddress + link.get('href') + \"?show=full\"\n",
    "\n",
    "            thesis = {\n",
    "                \"name\" : link.string,\n",
    "                \"link\" : temp_addr,\n",
    "                \"type\" : th_type,\n",
    "                \"fac_name\" : fac_name,\n",
    "                \"dep_name\" : dep_name\n",
    "            }\n",
    "            data_frame = putDataIntoFrame(thesis,data_frame)\n",
    "            theses.append(thesis)\n",
    "            num_of_th_on_page+=1\n",
    "        \n",
    "        #if there was no theses on page we just stop loop\n",
    "        if num_of_th_on_page == 0 :\n",
    "            break\n",
    "            \n",
    "        offset+=20 #there is 20 records on one page\n",
    "\n",
    "temp_filename = file_to_save.format(\"all_data\")\n",
    "\n",
    "data_frame.to_csv(temp_filename, sep='|', encoding='utf-8')\n",
    "\n",
    "print('successfully saved ',temp_filename)\n",
    "\n",
    "print()\n",
    "print(\"--------------------\")\n",
    "print(\"END OF ALGORITHM !!!\")\n",
    "print(\"--------------------\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
