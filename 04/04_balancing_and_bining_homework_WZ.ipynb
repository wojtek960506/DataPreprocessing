{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_names = [0,1] # for printing purposes later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing the task (a.k.a homework)\n",
    "\n",
    "Try to improve the performance of a classifier on `data` with the following conditions:\n",
    "\n",
    "1. use binning (on features of your choice, with your choice of parameters) and comment on its effects on classification\n",
    "1. use at least 2 other preprocessing techniques (your choice!) on the data set and comment the classification results\n",
    "1. run all classification test at least twice - once for unbalanced original data, once for balanced data (choose a balancing technique), compare those results (give a comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA AND IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, ttest_ind\n",
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, RFECV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_log_error, make_scorer, mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import OneSidedSelection \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('default_of_credit_card_clients.csv',sep=';')\n",
    "raw_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I copied data into raw_data to compare it in the end with the preprocessed and balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION TO CHECK CLASSIFIERS RESULTS\n",
    "\n",
    "I decided to check data with two classifiers with which I get the best AUC - Decision Tree and AdaBoost\n",
    "In this function I for each classifier I print its name, confusion matrix and Area Under Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_classifiers(data, plot = False):\n",
    "    names = [\"DECISION TREE\",\n",
    "             \"ADABOOST\",\n",
    "            ]\n",
    "\n",
    "    classifiers = [\n",
    "        DecisionTreeClassifier(max_depth=5),\n",
    "        AdaBoostClassifier()        \n",
    "    ]\n",
    "\n",
    "    for name, clf in zip(names, classifiers):\n",
    "\n",
    "        print(name)\n",
    "\n",
    "        y = list(data['class'])\n",
    "        X = data.drop(['class'],axis=1)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "        y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        print(cnf_matrix)\n",
    "        cnf_matrix_m = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        print(cnf_matrix_m)\n",
    "        cnf_acc = accuracy_score(y_test,y_pred)\n",
    "        print(\"Accuracy = \" + str(cnf_acc))\n",
    "        probas_ = clf.fit(X_train, y_train).predict_proba(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print(\"AUC = \" + str(roc_auc))\n",
    "        print() \n",
    "        print()\n",
    "        \n",
    "        if plot:\n",
    "            np.set_printoptions(precision=2)\n",
    "            # Plot non-normalized confusion matrix\n",
    "            plt.figure()\n",
    "            plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                                  title='Confusion matrix, without normalization')\n",
    "            # Plot normalized confusion matrix\n",
    "            plt.figure()\n",
    "            plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                                 title='Normalized confusion matrix')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE\n",
      "[[8886  471]\n",
      " [1685  958]]\n",
      "[[0.94966335 0.05033665]\n",
      " [0.63753311 0.36246689]]\n",
      "Accuracy = 0.8203333333333334\n",
      "AUC = 0.7488825663447612\n",
      "\n",
      "\n",
      "ADABOOST\n",
      "[[8930  427]\n",
      " [1765  878]]\n",
      "[[0.95436572 0.04563428]\n",
      " [0.66780174 0.33219826]]\n",
      "Accuracy = 0.8173333333333334\n",
      "AUC = 0.7737218026399817\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_classifiers(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROBUST SCALER\n",
    "\n",
    "I want to detect and delete outliers from my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# we rescale only numeric values - not indicators\n",
    "columns = data.select_dtypes(include=['int64']).columns \n",
    "columns = columns.drop('class', errors = 'ignore')\n",
    "\n",
    "robust_scaler.fit(data[columns])\n",
    "\n",
    "data[columns] = robust_scaler.transform(data[columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE\n",
      "[[8886  471]\n",
      " [1685  958]]\n",
      "[[0.94966335 0.05033665]\n",
      " [0.63753311 0.36246689]]\n",
      "Accuracy = 0.8203333333333334\n",
      "AUC = 0.7492607625280973\n",
      "\n",
      "\n",
      "ADABOOST\n",
      "[[8930  427]\n",
      " [1765  878]]\n",
      "[[0.95436572 0.04563428]\n",
      " [0.66780174 0.33219826]]\n",
      "Accuracy = 0.8173333333333334\n",
      "AUC = 0.7737218026399817\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_classifiers(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that this preprocessing technique make almost no changes with the classification of data. It even make classification with Decision Tree event very little worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BINING\n",
    "\n",
    "Bining can balanced our data. I have chosen to do bining with the columns which describe \"Amount of previous payment\", because I think that there are values, which have good variability for bining. It might be also good idea to do to binning with colums which describe credit value and year, but I checked that they make totally no changes with the classification results, so I decide to do binig just with columns mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1 = 50 #number of bining sets\n",
    "labels1 = [x+1 for x in range(val1)]\n",
    "for i in range(18,24):\n",
    "    tmp = str(i)\n",
    "    tmp1 = 'X'+tmp\n",
    "    data[tmp1] = pd.cut(data[tmp1],val1,labels = labels1)\n",
    "    data[tmp1] = data[tmp1].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE\n",
      "[[8922  435]\n",
      " [1719  924]]\n",
      "[[0.95351074 0.04648926]\n",
      " [0.65039728 0.34960272]]\n",
      "Accuracy = 0.8205\n",
      "AUC = 0.7526707350758177\n",
      "\n",
      "\n",
      "ADABOOST\n",
      "[[8944  413]\n",
      " [1766  877]]\n",
      "[[0.95586192 0.04413808]\n",
      " [0.6681801  0.3318199 ]]\n",
      "Accuracy = 0.8184166666666667\n",
      "AUC = 0.7704135261685031\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_classifiers(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I can see that bining make decision tree classification better. But still those changes are not very significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIN-MAX NORMALIZATION\n",
    "\n",
    "I will try another normalization technique to have normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to be standardized:  ['X18', 'X19', 'X20', 'X21', 'X22', 'X23']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# Min-max normalization\n",
    "minmax_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# we rescale only numeric values - not indicators\n",
    "columns = data.select_dtypes(include=['int64']).columns \n",
    "columns = columns.drop('class', errors = 'ignore')\n",
    "print('Columns to be standardized: ', list(columns))\n",
    "\n",
    "# Prepare values for the transformation\n",
    "minmax_scaler.fit(data[columns])\n",
    "\n",
    "# MinMax\n",
    "data[columns] = minmax_scaler.transform(data[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE\n",
      "[[8923  434]\n",
      " [1720  923]]\n",
      "[[0.95361761 0.04638239]\n",
      " [0.65077563 0.34922437]]\n",
      "Accuracy = 0.8205\n",
      "AUC = 0.753578438264477\n",
      "\n",
      "\n",
      "ADABOOST\n",
      "[[8944  413]\n",
      " [1766  877]]\n",
      "[[0.95586192 0.04413808]\n",
      " [0.6681801  0.3318199 ]]\n",
      "Accuracy = 0.8184166666666667\n",
      "AUC = 0.7704135261685031\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_classifiers(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still very similar results as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23364\n",
       "1     6636\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is unbalanced so we need to balance it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BALANCING\n",
    "\n",
    "Now with the preprocessed data I will try do to some balancing. I will check one under-sampling method, one over-sampling method and one method which use over-sampling and under-sampling to compare results. As before I will use two classifiers - Decide Tree and AdaBoost and print for each of them its name, confusion matrix, normalized confusion matrix and Area Under Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE DECISION TREE DECISION TREE \n",
      "ENN\n",
      "Wall time: 19.9 s\n",
      "Counter({0: 14107, 1: 6636})\n",
      "[[5421  226]\n",
      " [1257 1394]]\n",
      "[[0.95997875 0.04002125]\n",
      " [0.47416069 0.52583931]]\n",
      "Accuracy = 0.8212822366835382\n",
      "AUC = 0.8176113180073715\n",
      "\n",
      "\n",
      "SMOTE\n",
      "Wall time: 1.05 s\n",
      "Counter({1: 23364, 0: 23364})\n",
      "[[7777 1533]\n",
      " [3775 5607]]\n",
      "[[0.83533835 0.16466165]\n",
      " [0.40236623 0.59763377]]\n",
      "Accuracy = 0.7160282473785576\n",
      "AUC = 0.778043524851963\n",
      "\n",
      "\n",
      "SMOTEENN\n",
      "Wall time: 44 s\n",
      "Counter({1: 17386, 0: 12251})\n",
      "[[4579  323]\n",
      " [2240 4713]]\n",
      "[[0.93410853 0.06589147]\n",
      " [0.3221631  0.6778369 ]]\n",
      "Accuracy = 0.7838043019822859\n",
      "AUC = 0.865803987993524\n",
      "\n",
      "\n",
      "ADABOOST ADABOOST ADABOOST \n",
      "ENN\n",
      "Wall time: 19.5 s\n",
      "Counter({0: 14107, 1: 6636})\n",
      "[[5365  282]\n",
      " [1233 1418]]\n",
      "[[0.95006198 0.04993802]\n",
      " [0.46510751 0.53489249]]\n",
      "Accuracy = 0.8174258857556037\n",
      "AUC = 0.8327548728984662\n",
      "\n",
      "\n",
      "SMOTE\n",
      "Wall time: 1.05 s\n",
      "Counter({1: 23364, 0: 23364})\n",
      "[[7716 1594]\n",
      " [3133 6249]]\n",
      "[[0.82878625 0.17121375]\n",
      " [0.33393733 0.66606267]]\n",
      "Accuracy = 0.7471110635566017\n",
      "AUC = 0.8213233410138617\n",
      "\n",
      "\n",
      "SMOTEENN\n",
      "Wall time: 39.3 s\n",
      "Counter({1: 17386, 0: 12251})\n",
      "[[4273  629]\n",
      " [1444 5509]]\n",
      "[[0.87168503 0.12831497]\n",
      " [0.20768014 0.79231986]]\n",
      "Accuracy = 0.8251370729649937\n",
      "AUC = 0.9124260355550409\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 5)\n",
    "\n",
    "\n",
    "y = list(data['class'])\n",
    "X = data.drop(['class'],axis=1)\n",
    "\n",
    "\n",
    "names = [\"DECISION TREE\",\n",
    "         \"ADABOOST\"\n",
    "        ]\n",
    "\n",
    "classifiers = [\n",
    "        DecisionTreeClassifier(max_depth=5),\n",
    "        AdaBoostClassifier()\n",
    "    ]\n",
    "\n",
    "resamplers = [\n",
    "    (EditedNearestNeighbours(random_state=42),'ENN'),\n",
    "    (SMOTE(random_state=42),'SMOTE'),\n",
    "    (SMOTEENN(random_state=42),'SMOTEENN')\n",
    "]\n",
    "\n",
    "for name,clf in zip(names,classifiers):\n",
    "    print(str(name + \" \") * 3)\n",
    "\n",
    "    for resampler,description in resamplers:\n",
    "        print(description)\n",
    "        %time X_res, y_res = resampler.fit_sample(X, y)\n",
    "        print(Counter(y_res))\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=.4, random_state=42)\n",
    "        y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "        \n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        print(cnf_matrix)\n",
    "        cnf_matrix_m = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        print(cnf_matrix_m)\n",
    "        cnf_acc = accuracy_score(y_test,y_pred)\n",
    "        print(\"Accuracy = \" + str(cnf_acc))\n",
    "        probas_ = clf.fit(X_train, y_train).predict_proba(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print(\"AUC = \" + str(roc_auc))\n",
    "        print() \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I will check classification results for not balanced and not preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE\n",
      "[[8885  472]\n",
      " [1685  958]]\n",
      "[[0.94955648 0.05044352]\n",
      " [0.63753311 0.36246689]]\n",
      "Accuracy = 0.82025\n",
      "AUC = 0.7485120731842975\n",
      "\n",
      "\n",
      "ADABOOST\n",
      "[[8930  427]\n",
      " [1765  878]]\n",
      "[[0.95436572 0.04563428]\n",
      " [0.66780174 0.33219826]]\n",
      "Accuracy = 0.8173333333333334\n",
      "AUC = 0.7737218026399817\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_classifiers(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that the balanced data I have better results than unbalanced data and better than for not preprocessed data.\n",
    "\n",
    "The best result is get for SMOTEENN balancing technique with the AdaBoost classifier. So it proves that data preprocesing helps with training data and that combining over-sampling and under-sampling methods deliver the best results for data classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
